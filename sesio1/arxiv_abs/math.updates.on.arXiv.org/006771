The convergence of many numerical optimization techniques is highly sensitive to the initial guess
provided to the solver. We propose an approach based on tensor methods to initialize the existing
optimization solvers close to global optima. The approach uses only the definition of the cost function
and does not need access to any database of good solutions. We first transform the cost function,
which is a function of task parameters and optimization variables, into a probability density function.
Unlike existing approaches that set the task parameters as constant, we consider them as another
set of random variables and approximate the joint probability distribution of the task parameters
and the optimization variables using a surrogate probability model. For a given task, we then generate
samples from the conditional distribution with respect to the given task parameter and use them
as initialization for the optimization solver. As conditioning and sampling from an arbitrary
density function are challenging, we use Tensor Train decomposition to obtain a surrogate probability
model from which we can efficiently obtain the conditional model and the samples. The method can
produce multiple solutions coming from different modes (when they exist) for a given task. We first
evaluate the approach by applying it to various challenging benchmark functions for numerical
optimization that are difficult to solve using gradient-based optimization solvers with a naive
initialization, showing that the proposed method can produce samples close to the global optima
and coming from multiple modes. We then demonstrate the generality of the framework and its relevance
to robotics by applying the proposed method to inverse kinematics and motion planning problems
with a 7-DoF manipulator. 