In stochastic optimal control, change of measure arguments have been crucial for stochastic analysis.
Such an approach is often called static reduction in dynamic team theory and has been an effective
method for establishing existence and approximation results for optimal policies. These arguments
have also been applied to the study of dynamic games. In this paper, we demonstrate the limitations
of such an approach for a wide class of stochastic dynamic games, where additionally, unlike the
team setting considered in Part I, informational dependence of equilibrium behavior is significantly
more complicated. We identify three types of static reductions: (i) those that are policy-independent
(as those introduced by Witsenhausen for teams), (ii) those that are policy-dependent (as those
introduced by Ho and Chu for partially nested dynamic teams), and (iii) a third type that we will refer
to as static measurements with control-sharing reduction as in Part I (where the measurements are
static although control actions are shared according to the partially nested information structure).
For the first type, we show that there is a bijection between Nash equilibrium policies under the
original information structure and their policy-independent static reductions. However, for
the second type, we show that there is generally no isomorphism between Nash equilibrium solutions
under the original information structure and their policy-dependent static reductions. Sufficient
conditions are presented to establish such an isomorphism relationship between Nash equilibria
of dynamic non-zero-sum games and their policy-dependent static reductions. For zero-sum games,
these sufficient conditions are relaxed and stronger results are established. We also study three
classes of multi-stage games, where we establish connections between closed-loop, open-loop,
and control-sharing Nash and saddle point equilibria. 