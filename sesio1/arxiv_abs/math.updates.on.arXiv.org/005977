The tensor Ising model is a discrete exponential family used for modeling binary data on networks
with not just pairwise, but higher-order dependencies. Here, the sufficient statistic is a multi-linear
form of degree $p\ge 2$, designed to capture $p$-fold interactions between the binary variables
sitting on the nodes of a network. A particularly useful class of tensor Ising models are the tensor
Curie-Weiss models, where all $p$-tuples of nodes interact with the same intensity. Computing
the maximum likelihood estimator (MLE) is computationally cumbersome in this model, due to the
presence of an inexplicit normalizing constant in the likelihood. The standard alternative is
to use the maximum pseudolikelihood estimator (MPLE). Both the MLE and the MPLE are consistent estimators
of the natural parameter, provided the latter lies strictly above a certain threshold, which is
slightly below $\log 2$. In this paper, we compute the Bahadur efficiencies of the MLE and the MPLE
above the threshold, and derive the optimal sample size (number of nodes) needed for either of these
tests to achieve significance. We show that the optimal sample size for the MPLE and the MLE agree
if either $p=2$ or the null parameter is greater than or equal to $\log 2$. On the other hand, if $p\ge
3$ and the null parameter lies strictly between the threshold and $\log 2$, then the two differ for
sufficiently large values of the alternative. For every fixed alternative above the threshold,
the Bahadur asymptotic relative efficiency of the MLE with respect to the MPLE goes to $\infty$ as
the null parameter approaches the threshold. Finally, we show a universality phenomenon, which
says that these results extend beyond the tensor Curie-Weiss model, and hold for the more general
class of Erd\H{o}s-R\'enyi hypergraph Ising models, where we can even allow for some sparsity in
the underlying Erd\H{o}s-R\'enyi hypergraph. 