Recently, saddle point problems have received much attention due to their powerful modeling capability
for a lot of problems from diverse domains. Applications of these problems occur in many applied
areas, such as robust optimization, distributed optimization, game theory, and many applications
in machine learning such as empirical risk minimization and generative adversarial networks training.
Therefore, many researchers have actively worked on developing numerical methods for solving
saddle point problems in many different settings. This paper is devoted to developing a numerical
method for solving saddle point problems in the non-convex uniformly-concave setting. We study
a general class of saddle point problems with composite structure and H\"older-continuous higher-order
derivatives. To solve the problem under consideration, we propose an approach in which we reduce
the problem to a combination of two auxiliary optimization problems separately for each group of
variables, outer minimization problem w.r.t. primal variables, and inner maximization problem
w.r.t the dual variables. For solving the outer minimization problem, we use the \textit{Adaptive
Gradient Method}, which is applicable for non-convex problems and also works with an inexact oracle
that is generated by approximately solving the inner problem. For solving the inner maximization
problem, we use the \textit{Restarted Unified Acceleration Framework}, which is a framework that
unifies the high-order acceleration methods for minimizing a convex function that has H\"older-continuous
higher-order derivatives. Separate complexity bounds are provided for the number of calls to the
first-order oracles for the outer minimization problem and higher-order oracles for the inner
maximization problem. Moreover, the complexity of the whole proposed approach is then estimated.
