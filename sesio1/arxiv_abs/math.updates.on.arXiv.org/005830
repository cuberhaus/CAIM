We introduce a notion called entropic independence that is an entropic analog of spectral notions
of high-dimensional expansion. Informally, entropic independence of a background distribution
$\mu$ on $k$-sized subsets of a ground set of elements says that for any (possibly randomly chosen)
set $S$, the relative entropy of a single element of $S$ drawn uniformly at random carries at most
$O(1/k)$ fraction of the relative entropy of $S$. Entropic independence is the analog of the notion
of spectral independence, if one replaces variance by entropy. We use entropic independence to
derive tight mixing time bounds, overcoming the lossy nature of spectral analysis of Markov chains
on exponential-sized state spaces. In our main technical result, we show a general way of deriving
entropy contraction, a.k.a. modified log-Sobolev inequalities, for down-up random walks from
spectral notions. We show that spectral independence of a distribution under arbitrary external
fields automatically implies entropic independence. To derive our results, we relate entropic
independence to properties of polynomials: $\mu$ is entropically independent exactly when a transformed
version of the generating polynomial of $\mu$ is upper bounded by its linear tangent; this property
is implied by concavity of the said transformation, which was shown by prior work to be locally equivalent
to spectral independence. We apply our results to obtain tight modified log-Sobolev inequalities
and mixing times for multi-step down-up walks on fractionally log-concave distributions. As our
flagship application, we establish the tight mixing time of $O(n\log n)$ for Glauber dynamics on
Ising models whose interaction matrix has eigenspectrum lying within an interval of length smaller
than $1$, improving upon the prior quadratic dependence on $n$. 