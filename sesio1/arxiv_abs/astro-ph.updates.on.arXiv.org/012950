Solar flares release an enormous amount of energy in the corona. A substantial fraction of this energy
is transported to the lower atmosphere, which results in chromospheric heating. The mechanisms
that transport energy to the lower solar atmosphere during a flare are still not fully understood.
We aim to estimate the temporal evolution of the radiative losses in the chromosphere at the footpoints
of a C-class flare, in order to set observational constraints on the electron beam parameters of
a RADYN flare simulation. We estimated the radiative losses from hydrogen, and singly ionized Ca
and Mg using semi-empirical model atmospheres. To estimate the integrated radiative losses in
the chromosphere the net cooling rates were integrated between the temperature minimum and the
height where the temperature reaches 10 kK. The stratification of the net cooling rate suggests
that the Ca IR triplet lines are responsible for most of the radiative losses in the flaring atmosphere.
During the flare peak time, the contribution from Ca II H & K and Mg II h & k lines are strong and comparable
to the Ca IR triplet ($\sim$32 kW m$^{-2}$). Since our flare is a relatively weak event the chromosphere
is not heated above 11 kK, which in turn yields a subdued Ly{\alpha} contribution ($\sim$7 kW m$^{-2}$).
The temporal evolution of total integrated radiative losses exhibits sharply-rising losses (0.4
kW m$^{-2}$ s$^{-1}$) and a relatively slow decay (0.23 kW~m$^{-2}$ s$^{-1}$). The maximum value
of total radiative losses is reached around the flare peak time, and can go up to 175 kW m$^{-2}$ for
a single pixel located at footpoint. After a small parameter study, we find the best model-data consistency
in terms of the amplitude of radiative losses and the overall atmospheric structure with a RADYN
flare simulation in the injected energy flux of $5\times10^{10}$ erg s$^{-1}$ cm$^{-2}$. 