Our ability to calibrate current kilometer-scale interferometers can potentially confound the
inference of astrophysical signals. Current calibration uncertainties are well described by
a Gaussian process. I exploit this description to analytically examine the impact of calibration
uncertainty. I derive closed-form expressions for the conditioned likelihood of the calibration
error given the observed data and an astrophysical signal (astrophysical calibration) as well
as for the marginal likelihood for the data given a signal (integrated over the calibration uncertainty).
I show that calibration uncertainty always reduces search sensitivity and the amount of information
available about astrophysical signals. Additionally, calibration uncertainty will fundamentally
limit the precision to which loud signals can be constrained, a crucial factor when considering
the scientific potential of proposed third-generation interferometers. For example, I estimate
that with $1\%$ uncertainty in the detector response's amplitude and phase, one will only be able
to measure the leading-order tidal parameter ($\tilde\Lambda$) for a 1.4+1.4$\,M_\odot$ system
to better than $\pm 1$ ($\sim 0.2\%$ relative uncertainty) for signals with signal-to-noise ratios
$\gtrsim 10^4$. At this signal-to-noise ratio, calibration uncertainty increases $\sigma_{\tilde\Lambda}$
by a factor of $2$ compared to stationary Gaussian noise alone. Furthermore, 1\% calibration uncertainty
limits the precision to always be $\sigma_{\tilde\Lambda} \gtrsim 0.5$. I also show how to best
select the frequencies at which calibration should be precisely constrained in order to minimize
the information lost about astrophysical parameters. It is not necessary to constrain the calibration
errors to be small at all frequencies to perform precise astrophysical inference for individual
signals. 