(abridged) When stripped from their hydrogen-rich envelopes, stars with initial masses between
$\sim$7 and 11 M$_\odot$ develop massive degenerate cores and collapse. Depending on the final
structure and composition, the outcome can range from a thermonuclear explosion, to the formation
of a neutron star in an electron-capture supernova (ECSN). It has been recently demonstrated that
stars in this mass range may initiate explosive oxygen burning when their central densities are
still below $\rho_{\rm c} \lesssim 10^{9.6}$ g cm$^{-3}$. This makes them interesting candidates
for Type Ia Supernovae -- which we call (C)ONe SNe Ia -- and might have broader implications for the
formation of neutron stars via ECSNe. Here, we model the evolution of 252 helium-stars with initial
masses in the $0.8-3.5$ M$_\odot$ range, and metallicities between $Z=10^{-4}$ and $0.02$. We
use these models to constrain the central densities, compositions and envelope masses at the time
of explosive oxygen ignition. We further investigate the sensitivity of these properties to mass-loss
rate assumptions using additional models with varying wind efficiencies. We find that helium-stars
with masses between $\sim$1.8 and 2.7 M$_\odot$ evolve onto $1.35-1.37$ M$_\odot$ (C)ONe cores
that initiate explosive burning at central densities between $\rm \log_{10}(\rho_c)\sim 9.3$
and 9.6. We constrain the amount of residual carbon retained after core carbon burning, and conclude
that it plays a critical role in determining the final outcome: Cores with residual carbon mass fractions
of $X_{\rm min}(\rm{{^{12}}C}) \gtrsim 0.004$ result in (C)ONe SNe Ia, while those with lower carbon
mass fractions become ECSNe. We find that (C)ONe SNe Ia are more likely to occur at high metallicities,
whereas at low metallicities ECSNe dominate. 