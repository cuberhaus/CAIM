Future high-resolution imaging X-ray observatories may require detectors with both fine spatial
resolution and high quantum efficiency at relatively high X-ray energies (>5keV). A silicon imaging
detector meeting these requirements will have a ratio of detector thickness to pixel size of six
or more, roughly twice that of legacy imaging sensors. This implies greater diffusion of X-ray charge
packets. We investigate consequences for sensor performance, reporting charge diffusion measurements
in a fully-depleted, 50um thick, back-illuminated CCD with 8um pixels. We are able to measure the
size distributions of charge packets produced by 5.9 keV and 1.25 keV X-rays in this device. We find
that individual charge packets exhibit a gaussian spatial distribution, and determine the frequency
distribution of event widths for a range of internal electric field strength levels. We find a standard
deviation for the largest charge packets, which occur near the entrance window, of 3.9um. We show
that the shape of the event width distribution provides a clear indicator of full depletion and infer
the relationship between event width and interaction depth. We compare measured width distributions
to simulations. We compare traditional, 'sum-above-threshold' algorithms for event amplitude
determination to 2D gaussian fitting of events and find better spectroscopic performance with
the former for 5.9 keV events and comparable results at 1.25 keV. The reasons for this difference
are discussed. We point out the importance of read noise driven detection thresholds in spectral
resolution, and note that the derived read noise requirements for mission concepts such as AXIS
and Lynx may be too lax to meet spectral resolution requirements. While we report measurements made
with a CCD, we note that they have implications for the performance of high aspect-ratio silicon
active pixel sensors as well. 