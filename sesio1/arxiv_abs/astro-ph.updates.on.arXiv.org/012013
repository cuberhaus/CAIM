The sensitivity of gravitational-waves detectors is characterized by their noise curves which
determine the detector's reach and the ability to accurately measure the parameters of astrophysical
sources. The detector noise is typically modelled as stationary and Gaussian for many practical
purposes. However, physical changes in the state of detectors due to environmental and instrumental
factors, including extreme cases where a detector discontinues observing for some time, introduce
non-stationarity into the noise. Even slow evolution of the detector sensitivity will affect long
duration signals such as binary neutron star (BNS) mergers. Mis-estimation of the noise behavior
directly impacts the posterior width of the signal parameters. This becomes an issue for studies
which depend on accurate localization volumes such as i) probing cosmological parameters (such
as Hubble constant, clustering bias) using cross-correlation methods with galaxies, ii) doing
electromagnetic follow-up using localization information from parameter estimation done from
pre-merger data. We study the effects of dynamical noise on the parameter estimation of the GW events.
We develop a new method to correct dynamical noise by estimating a locally-valid pseudo PSD which
is normalized along the time-frequency track of a potential signal. We do simulations by injecting
the BNS signal in various scenarios where the detector goes through a period of non-stationarity
with reference noise curve of third generation detectors (Cosmic explorer, Einstein telescope).
As an example, for a source where mis-modelling of the noise biases the signal-to-noise estimate
by even $10\%$, one would expect the estimated localization volume to be either under or over reported
by $\sim 30\%$; errors like this, especially in low-latency, could potentially cause follow-up
campaigns to miss the true source location. 