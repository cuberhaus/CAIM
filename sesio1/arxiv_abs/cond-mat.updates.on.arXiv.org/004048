A complex interplay of single-neuron properties and the recurrent network structure shapes the
activity of cortical neurons. The single-neuron activity statistics differ in general from the
respective population statistics, including spectra and, correspondingly, autocorrelation
times. We develop a theory for self-consistent second-order single-neuron statistics in block-structured
sparse random networks of spiking neurons. In particular, the theory predicts the neuron-level
autocorrelation times, also known as intrinsic timescales, of the neuronal activity. The theory
is based on an extension of dynamic mean-field theory from rate networks to spiking networks, which
is validated via simulations. It accounts for both static variability, e.g. due to a distributed
number of incoming synapses per neuron, and temporal fluctuations of the input. We apply the theory
to balanced random networks of generalized linear model neurons, balanced random networks of leaky
integrate-and-fire neurons, and a biologically constrained network of leaky integrate-and-fire
neurons. For the generalized linear model network with an error function nonlinearity, a novel
analytical solution of the colored noise problem allows us to obtain self-consistent firing rate
distributions, single-neuron power spectra, and intrinsic timescales. For the leaky integrate-and-fire
networks, we derive an approximate analytical solution of the colored noise problem, based on the
Stratonovich approximation of the Wiener-Rice series and a novel analytical solution for the free
upcrossing statistics. Again closing the system self-consistently, in the fluctuation-driven
regime this approximation yields reliable estimates of the mean firing rate and its variance across
neurons, the inter-spike interval distribution, the single-neuron power spectra, and intrinsic
timescales. 