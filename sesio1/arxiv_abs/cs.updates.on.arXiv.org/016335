Deep learning models have been deployed in an increasing number of edge and mobile devices to provide
healthcare. These models rely on training with a tremendous amount of labeled data to achieve high
accuracy. However, for medical applications such as dermatological disease diagnosis, the private
data collected by mobile dermatology assistants exist on distributed mobile devices of patients,
and each device only has a limited amount of data. Directly learning from limited data greatly deteriorates
the performance of learned models. Federated learning (FL) can train models by using data distributed
on devices while keeping the data local for privacy. Existing works on FL assume all the data have
ground-truth labels. However, medical data often comes without any accompanying labels since
labeling requires expertise and results in prohibitively high labor costs. The recently developed
self-supervised learning approach, contrastive learning (CL), can leverage the unlabeled data
to pre-train a model, after which the model is fine-tuned on limited labeled data for dermatological
disease diagnosis. However, simply combining CL with FL as federated contrastive learning (FCL)
will result in ineffective learning since CL requires diverse data for learning but each device
only has limited data. In this work, we propose an on-device FCL framework for dermatological disease
diagnosis with limited labels. Features are shared in the FCL pre-training process to provide diverse
and accurate contrastive information. After that, the pre-trained model is fine-tuned with local
labeled data independently on each device or collaboratively with supervised federated learning
on all devices. Experiments on dermatological disease datasets show that the proposed framework
effectively improves the recall and precision of dermatological disease diagnosis compared with
state-of-the-art methods. 