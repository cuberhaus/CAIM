Objective: Disease knowledge graphs are a way to connect, organize, and access disparate information
about diseases with numerous benefits for artificial intelligence (AI). To create knowledge graphs,
it is necessary to extract knowledge from multimodal datasets in the form of relationships between
disease concepts and normalize both concepts and relationship types. Methods: We introduce REMAP,
a multimodal approach for disease relation extraction and classification. The REMAP machine learning
approach jointly embeds a partial, incomplete knowledge graph and a medical language dataset into
a compact latent vector space, followed by aligning the multimodal embeddings for optimal disease
relation extraction. Results: We apply REMAP approach to a disease knowledge graph with 96,913
relations and a text dataset of 1.24 million sentences. On a dataset annotated by human experts,
REMAP improves text-based disease relation extraction by 10.0% (accuracy) and 17.2% (F1-score)
by fusing disease knowledge graphs with text information. Further, REMAP leverages text information
to recommend new relationships in the knowledge graph, outperforming graph-based methods by 8.4%
(accuracy) and 10.4% (F1-score). Discussion: Systematized knowledge is becoming the backbone
of AI, creating opportunities to inject semantics into AI and fully integrate it into machine learning
algorithms. While prior semantic knowledge can assist in extracting disease relationships from
text, existing methods can not fully leverage multimodal datasets. Conclusion: REMAP is a multimodal
approach for extracting and classifying disease relationships by fusing structured knowledge
and text information. REMAP provides a flexible neural architecture to easily find, access, and
validate AI-driven relationships between disease concepts. 