Dealing with uncertain, contradicting, and ambiguous information is still a central issue in Artificial
Intelligence (AI). As a result, many formalisms have been proposed or adapted so as to consider non-monotonicity,
with only a limited number of works and researchers performing any sort of comparison among them.
A non-monotonic formalism is one that allows the retraction of previous conclusions or claims,
from premises, in light of new evidence, offering some desirable flexibility when dealing with
uncertainty. This research article focuses on evaluating the inferential capacity of defeasible
argumentation, a formalism particularly envisioned for modelling non-monotonic reasoning.
In addition to this, fuzzy reasoning and expert systems, extended for handling non-monotonicity
of reasoning, are selected and employed as baselines, due to their vast and accepted use within the
AI community. Computational trust was selected as the domain of application of such models. Trust
is an ill-defined construct, hence, reasoning applied to the inference of trust can be seen as non-monotonic.
Inference models were designed to assign trust scalars to editors of the Wikipedia project. In particular,
argument-based models demonstrated more robustness than those built upon the baselines despite
the knowledge bases or datasets employed. This study contributes to the body of knowledge through
the exploitation of defeasible argumentation and its comparison to similar approaches. The practical
use of such approaches coupled with a modular design that facilitates similar experiments was exemplified
and their respective implementations made publicly available on GitHub [120, 121]. This work adds
to previous works, empirically enhancing the generalisability of defeasible argumentation as
a compelling approach to reason with quantitative data and uncertain knowledge. 