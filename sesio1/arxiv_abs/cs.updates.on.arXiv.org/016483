The success or failure of modern computer-assisted surgery procedures hinges on the precise six-degree-of-freedom
(6DoF) position and orientation (pose) estimation of tracked instruments and tissue. In this paper,
we present HMD-EgoPose, a single-shot learning-based approach to hand and object pose estimation
and demonstrate state-of-the-art performance on a benchmark dataset for monocular red-green-blue
(RGB) 6DoF marker-less hand and surgical instrument pose tracking. Further, we reveal the capacity
of our HMD-EgoPose framework for 6DoF near real-time pose estimation on a commercially available
optical see-through head-mounted display (OST-HMD) through a low-latency streaming approach.
Our framework utilized an efficient convolutional neural network (CNN) backbone for multi-scale
feature extraction and a set of subnetworks to jointly learn the 6DoF pose representation of the
rigid surgical drill instrument and the grasping orientation of the hand of a user. To make our approach
accessible to a commercially available OST-HMD, the Microsoft HoloLens 2, we created a pipeline
for low-latency video and data communication with a high-performance computing workstation capable
of optimized network inference. HMD-EgoPose outperformed current state-of-the-art approaches
on a benchmark dataset for surgical tool pose estimation, achieving an average tool 3D vertex error
of 11.0 mm on real data and furthering the progress towards a clinically viable marker-free tracking
strategy. Through our low-latency streaming approach, we achieved a round trip latency of 202.5
ms for pose estimation and augmented visualization of the tracked model when integrated with the
OST-HMD. Our single-shot learned approach was robust to occlusion and complex surfaces and improved
on current state-of-the-art approaches to marker-less tool and hand pose estimation. 