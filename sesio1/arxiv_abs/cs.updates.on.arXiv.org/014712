Artificial Intelligence (AI) is one of the disruptive technologies that is shaping the future.
It has growing applications for data-driven decisions in major smart city solutions, including
transportation, education, healthcare, public governance, and power systems. At the same time,
it is gaining popularity in protecting critical cyber infrastructure from cyber threats, attacks,
damages, or unauthorized access. However, one of the significant issues of those traditional AI
technologies (e.g., deep learning) is that the rapid progress in complexity and sophistication
propelled and turned out to be uninterpretable black boxes. On many occasions, it is very challenging
to understand the decision and bias to control and trust systems' unexpected or seemingly unpredictable
outputs. It is acknowledged that the loss of control over interpretability of decision-making
becomes a critical issue for many data-driven automated applications. But how may it affect the
system's security and trustworthiness? This chapter conducts a comprehensive study of machine
learning applications in cybersecurity to indicate the need for explainability to address this
question. While doing that, this chapter first discusses the black-box problems of AI technologies
for Cybersecurity applications in smart city-based solutions. Later, considering the new technological
paradigm, Explainable Artificial Intelligence (XAI), this chapter discusses the transition
from black-box to white-box. This chapter also discusses the transition requirements concerning
the interpretability, transparency, understandability, and Explainability of AI-based technologies
in applying different autonomous systems in smart cities. Finally, it has presented some commercial
XAI platforms that offer explainability over traditional AI technologies before presenting future
challenges and opportunities. 