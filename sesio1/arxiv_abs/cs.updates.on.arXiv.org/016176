Federated learning (FL) allows mutually untrusted clients to collaboratively train a common machine
learning model without sharing their private/proprietary training data among each other. FL is
unfortunately susceptible to poisoning by malicious clients who aim to hamper the accuracy of the
commonly trained model through sending malicious model updates during FL's training process.
We argue that the key factor to the success of poisoning attacks against existing FL systems is the
large space of model updates available to the clients, allowing malicious clients to search for
the most poisonous model updates, e.g., by solving an optimization problem. To address this, we
propose Federated Rank Learning (FRL). FRL reduces the space of client updates from model parameter
updates (a continuous space of float numbers) in standard FL to the space of parameter rankings (a
discrete space of integer values). To be able to train the global model using parameter ranks (instead
of parameter weights), FRL leverage ideas from recent supermasks training mechanisms. Specifically,
FRL clients rank the parameters of a randomly initialized neural network (provided by the server)
based on their local training data. The FRL server uses a voting mechanism to aggregate the parameter
rankings submitted by clients in each training epoch to generate the global ranking of the next training
epoch. Intuitively, our voting-based aggregation mechanism prevents poisoning clients from
making significant adversarial modifications to the global model, as each client will have a single
vote! We demonstrate the robustness of FRL to poisoning through analytical proofs and experimentation.
We also show FRL's high communication efficiency. Our experiments demonstrate the superiority
of FRL in real-world FL settings. 