The exponential growth of scientific production makes secondary literature abridgements increasingly
demanding. We introduce a new open-source framework for systematic reviews that significantly
reduces time and workload for collecting and screening scientific literature. The framework provides
three main tools: 1) an automatic citation search engine and manager that collects records from
multiple online sources with a unified query syntax, 2) a Bayesian, active machine learning, citation
screening tool based on iterative human-machine interaction to increase predictive accuracy
and, 3) a semi-automatic, data-driven query generator to create new search queries from existing
citation data sets. To evaluate the automatic screener's performance, we estimated the median
posterior sensitivity and efficiency [90% Credible Intervals] using Bayesian simulation to predict
the distribution of undetected potentially relevant records. Tested on an example topic, the framework
collected 17,755 unique records through the citation manager; 766 records required human evaluation
while the rest were excluded by the automatic classifier; the theoretical efficiency was 95.6%
[95.3%, 95.7%] with a sensitivity of 100% [93.5%, 100%]. A new search query was generated from the
labelled dataset, and 82,579 additional records were collected; only 567 records required human
review after automatic screening, and six additional positive matches were found. The overall
expected sensitivity decreased to 97.3% [73.8%, 100%] while the efficiency increased to 98.6%
[98.2%, 98.7%]. The framework can significantly reduce the workload required to conduct large
literature reviews by simplifying citation collection and screening while demonstrating exceptional
sensitivity. Such a tool can improve the standardization and repeatability of systematic reviews.
