State-of-the-art approaches for metaphor detection compare their literal - or core - meaning and
their contextual meaning using sequential metaphor classifiers based on neural networks. The
signal that represents the literal meaning is often represented by (non-contextual) word embeddings.
However, metaphorical expressions evolve over time due to various reasons, such as cultural and
societal impact. Metaphorical expressions are known to co-evolve with language and literal word
meanings, and even drive, to some extent, this evolution. This rises the question whether different,
possibly time-specific, representations of literal meanings may impact on the metaphor detection
task. To the best of our knowledge, this is the first study which examines the metaphor detection
task with a detailed exploratory analysis where different temporal and static word embeddings
are used to account for different representations of literal meanings. Our experimental analysis
is based on three popular benchmarks used for metaphor detection and word embeddings extracted
from different corpora and temporally aligned to different state-of-the-art approaches. The
results suggest that different word embeddings do impact on the metaphor detection task and some
temporal word embeddings slightly outperform static methods on some performance measures. However,
results also suggest that temporal word embeddings may provide representations of words' core
meaning even too close to their metaphorical meaning, thus confusing the classifier. Overall,
the interaction between temporal language evolution and metaphor detection appears tiny in the
benchmark datasets used in our experiments. This suggests that future work for the computational
analysis of this important linguistic phenomenon should first start by creating a new dataset where
this interaction is better represented. 