There are two main algorithmic approaches to autonomous driving systems: (1) An end-to-end system
in which a single deep neural network learns to map sensory input directly into appropriate warning
and driving responses. (2) A mediated hybrid recognition system in which a system is created by combining
independent modules that detect each semantic feature. While some researchers believe that deep
learning can solve any problem, others believe that a more engineered and symbolic approach is needed
to cope with complex environments with less data. Deep learning alone has achieved state-of-the-art
results in many areas, from complex gameplay to predicting protein structures. In particular,
in image classification and recognition, deep learning models have achieved accuracies as high
as humans. But sometimes it can be very difficult to debug if the deep learning model doesn't work.
Deep learning models can be vulnerable and are very sensitive to changes in data distribution. Generalization
can be problematic. It's usually hard to prove why it works or doesn't. Deep learning models can also
be vulnerable to adversarial attacks. Here, we combine deep learning-based object recognition
and tracking with an adaptive neurosymbolic network agent, called the Non-Axiomatic Reasoning
System (NARS), that can adapt to its environment by building concepts based on perceptual sequences.
We achieved an improved intersection-over-union (IOU) object recognition performance of 0.65
in the adaptive retraining model compared to IOU 0.31 in the COCO data pre-trained model. We improved
the object detection limits using RADAR sensors in a simulated environment, and demonstrated the
weaving car detection capability by combining deep learning-based object detection and tracking
with a neurosymbolic model. 