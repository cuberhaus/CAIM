Affective computing plays a key role in human-computer interactions, entertainment, teaching,
safe driving, and multimedia integration. Major breakthroughs have been made recently in the areas
of affective computing (i.e., emotion recognition and sentiment analysis). Affective computing
is realized based on unimodal or multimodal data, primarily consisting of physical information
(e.g., textual, audio, and visual data) and physiological signals (e.g., EEG and ECG signals).
Physical-based affect recognition caters to more researchers due to multiple public databases.
However, it is hard to reveal one's inner emotion hidden purposely from facial expressions, audio
tones, body gestures, etc. Physiological signals can generate more precise and reliable emotional
results; yet, the difficulty in acquiring physiological signals also hinders their practical
application. Thus, the fusion of physical information and physiological signals can provide useful
features of emotional states and lead to higher accuracy. Instead of focusing on one specific field
of affective analysis, we systematically review recent advances in the affective computing, and
taxonomize unimodal affect recognition as well as multimodal affective analysis. Firstly, we
introduce two typical emotion models followed by commonly used databases for affective computing.
Next, we survey and taxonomize state-of-the-art unimodal affect recognition and multimodal affective
analysis in terms of their detailed architectures and performances. Finally, we discuss some important
aspects on affective computing and their applications and conclude this review with an indication
of the most promising future directions, such as the establishment of baseline dataset, fusion
strategies for multimodal affective analysis, and unsupervised learning models. 