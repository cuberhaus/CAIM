There are several ways to measure the compressibility of a random measure; they include general
approaches such as using the rate-distortion curve, as well as more specific notions, such as the
Renyi information dimension (RID). The RID parameter indicates the concentration of the measure
around lower-dimensional subsets of the space. While the evaluation of such compressibility parameters
is well-studied for continuous and discrete measures, the case of discrete-continuous measures
is quite subtle. In this paper, we focus on a class of multi-dimensional random measures that have
singularities on affine lower-dimensional subsets. This class of distributions naturally arises
when considering linear transformation of component-wise independent discrete-continuous
random variables. To measure the compressibility of such distributions, we introduce the new notion
of dimensional-rate bias (DRB) which is closely related to the entropy and differential entropy
in discrete and continuous cases, respectively. Similar to entropy and differential entropy,
DRB is useful in evaluating the mutual information between distributions of the aforementioned
type. Besides the DRB, we also evaluate the the RID of these distributions. We further provide an
upper-bound for the RID of multi-dimensional random measures that are obtained by Lipschitz functions
of component-wise independent discrete-continuous random variables ($\mathbf{X}$). The upper-bound
is shown to be achievable when the Lipschitz function is $A \mathbf{X}$, where $A$ satisfies {\changed$\spark({A_{m\times
n}}) = m+1$} (e.g., Vandermonde matrices). When considering discrete-domain moving-average
processes with non-Gaussian excitation noise, the above results allow us to evaluate the block-average
RID and DRB, as well as to determine a relationship between these parameters and other existing compressibility
measures. 