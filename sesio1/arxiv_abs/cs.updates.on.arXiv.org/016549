Natural Language Understanding (NLU) is a branch of Natural Language Processing (NLP) that uses
intelligent computer software to understand texts that encode human knowledge. Recent years have
witnessed notable progress across various NLU tasks with deep learning techniques, especially
with pretrained language models. Besides proposing more advanced model architectures, constructing
more reliable and trustworthy datasets also plays a huge role in improving NLU systems, without
which it would be impossible to train a decent NLU model. It's worth noting that the human ability
of understanding natural language is flexible and robust. On the contrary, most of existing NLU
systems fail to achieve desirable performance on out-of-domain data or struggle on handling challenging
items (e.g., inherently ambiguous items, adversarial items) in the real world. Therefore, in order
to have NLU models understand human language more effectively, it is expected to prioritize the
study on robust natural language understanding. In this thesis, we deem that NLU systems are consisting
of two components: NLU models and NLU datasets. As such, we argue that, to achieve robust NLU, the
model architecture/training and the dataset are equally important. Specifically, we will focus
on three NLU tasks to illustrate the robustness problem in different NLU tasks and our contributions
(i.e., novel models and new datasets) to help achieve more robust natural language understanding.
Moving forward, the ultimate goal for robust natural language understanding is to build NLU models
which can behave humanly. That is, it's expected that robust NLU systems are capable to transfer
the knowledge from training corpus to unseen documents more reliably and survive when encountering
challenging items even if the system doesn't know a priori of users' inputs. 