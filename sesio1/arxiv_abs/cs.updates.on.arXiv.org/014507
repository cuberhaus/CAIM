Modern handheld devices can acquire burst image sequence in a quick succession. However, the individual
acquired frames suffer from multiple degradations and are misaligned due to camera shake and object
motions. The goal of Burst Image Restoration is to effectively combine complimentary cues across
multiple burst frames to generate high-quality outputs. Towards this goal, we develop a novel approach
by solely focusing on the effective information exchange between burst frames, such that the degradations
get filtered out while the actual scene details are preserved and enhanced. Our central idea is to
create a set of \emph{pseudo-burst} features that combine complimentary information from all
the input burst frames to seamlessly exchange information. The pseudo-burst representations
encode channel-wise features from the original burst images, thus making it easier for the model
to learn distinctive information offered by multiple burst frames. However, the pseudo-burst
cannot be successfully created unless the individual burst frames are properly aligned to discount
inter-frame movements. Therefore, our approach initially extracts preprocessed features from
each burst frame and matches them using an edge-boosting burst alignment module. The pseudo-burst
features are then created and enriched using multi-scale contextual information. Our final step
is to adaptively aggregate information from the pseudo-burst features to progressively increase
resolution in multiple stages while merging the pseudo-burst features. In comparison to existing
works that usually follow a late fusion scheme with single-stage upsampling, our approach performs
favorably, delivering state of the art performance on burst super-resolution and low-light image
enhancement tasks. Our codes and models will be released publicly. 