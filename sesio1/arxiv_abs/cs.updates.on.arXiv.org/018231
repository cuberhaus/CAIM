An assembly is a large population of neurons whose synchronous firing is hypothesized to represent
a memory, concept, word, and other cognitive categories. Assemblies are believed to provide a bridge
between high-level cognitive phenomena and low-level neural activity. Recently, a computational
system called the Assembly Calculus (AC), with a repertoire of biologically plausible operations
on assemblies, has been shown capable of simulating arbitrary space-bounded computation, but
also of simulating complex cognitive phenomena such as language, reasoning, and planning. However,
the mechanism whereby assemblies can mediate learning has not been known. Here we present such a
mechanism, and prove rigorously that, for simple classification problems defined on distributions
of labeled assemblies, a new assembly representing each class can be reliably formed in response
to a few stimuli from the class; this assembly is henceforth reliably recalled in response to new
stimuli from the same class. Furthermore, such class assemblies will be distinguishable as long
as the respective classes are reasonably separated -- for example, when they are clusters of similar
assemblies. To prove these results, we draw on random graph theory with dynamic edge weights to estimate
sequences of activated vertices, yielding strong generalizations of previous calculations and
theorems in this field over the past five years. These theorems are backed up by experiments demonstrating
the successful formation of assemblies which represent concept classes on synthetic data drawn
from such distributions, and also on MNIST, which lends itself to classification through one assembly
per digit. Seen as a learning algorithm, this mechanism is entirely online, generalizes from very
few samples, and requires only mild supervision -- all key attributes of learning in a model of the
brain. 