This paper seeks to provide the information retrieval community with some reflections on the current
improvements of retrieval learning through the analysis of the reproducibility aspects of image-text
retrieval models. For the latter part of the past decade, image-text retrieval has gradually become
a major research direction in the field of information retrieval because of the growth of multi-modal
data. Many researchers use benchmark datasets like MS-COCO and Flickr30k to train and assess the
performance of image-text retrieval algorithms. Research in the past has mostly focused on performance,
with several state-of-the-art methods being proposed in various ways. According to their claims,
these approaches achieve better modal interactions and thus better multimodal representations
with greater precision. In contrast to those previous works, we focus on the repeatability of the
approaches and the overall examination of the elements that lead to improved performance by pretrained
and nonpretrained models in retrieving images and text. To be more specific, we first examine the
related reproducibility concerns and why the focus is on image-text retrieval tasks, and then we
systematically summarize the current paradigm of image-text retrieval models and the stated contributions
of those approaches. Second, we analyze various aspects of the reproduction of pretrained and nonpretrained
retrieval models. Based on this, we conducted ablation experiments and obtained some influencing
factors that affect retrieval recall more than the improvement claimed in the original paper. Finally,
we also present some reflections and issues that should be considered by the retrieval community
in the future. Our code is freely available at https://github.com/WangFei-2019/Image-text-Retrieval.
