Humans and animals excel in combining information from multiple sensory modalities, controlling
their complex bodies, adapting to growth, failures, or using tools. These capabilities are also
highly desirable in robots. They are displayed by machines to some extent - yet, as is so often the
case, the artificial creatures are lagging behind. The key foundation is an internal representation
of the body that the agent - human, animal, or robot - has developed. In the biological realm, evidence
has been accumulated by diverse disciplines giving rise to the concepts of body image, body schema,
and others. In robotics, a model of the robot is an indispensable component that enables to control
the machine. In this article I compare the character of body representations in biology with their
robotic counterparts and relate that to the differences in performance that we observe. I put forth
a number of axes regarding the nature of such body models: fixed vs. plastic, amodal vs. modal, explicit
vs. implicit, serial vs. parallel, modular vs. holistic, and centralized vs. distributed. An interesting
trend emerges: on many of the axes, there is a sequence from robot body models, over body image, body
schema, to the body representation in lower animals like the octopus. In some sense, robots have
a lot in common with Ian Waterman - "the man who lost his body" - in that they rely on an explicit, veridical
body model (body image taken to the extreme) and lack any implicit, multimodal representation (like
the body schema) of their bodies. I will then detail how robots can inform the biological sciences
dealing with body representations and finally, I will study which of the features of the "body in
the brain" should be transferred to robots, giving rise to more adaptive and resilient, self-calibrating
machines. 