For a long period, different recommendation tasks typically require designing task-specific
architectures and training objectives. As a result, it is hard to transfer the learned knowledge
and representations from one task to another, thus restricting the generalization ability of existing
recommendation approaches, e.g., a sequential recommendation model can hardly be applied or transferred
to a review generation method. To deal with such issues, considering that language grounding is
a powerful medium to describe and represent various problems or tasks, we present a flexible and
unified text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict Paradigm"
(P5) for recommendation, which unifies various recommendation tasks in a shared framework. In
P5, all data such as user-item interactions, item metadata, and user reviews are converted to a common
format -- natural language sequences. The rich information from natural language assist P5 to capture
deeper semantics for recommendation. P5 learns different tasks with the same language modeling
objective during pretraining. Thus, it possesses the potential to serve as the foundation model
for downstream recommendation tasks, allows easy integration with other modalities, and enables
instruction-based recommendation, which will revolutionize the technical form of recommender
system towards unified recommendation engine. With adaptive personalized prompt for different
users, P5 is able to make predictions in a zero-shot or few-shot manner and largely reduces the necessity
for extensive fine-tuning. On several recommendation benchmarks, we conduct experiments to show
the effectiveness of our generative approach. We will release our prompts and pretrained P5 language
model to help advance future research on Recommendation as Language Processing (RLP) and Personalized
Foundation Models. 