Clinical 12-lead electrocardiography (ECG) is one of the most widely encountered kinds of biosignals.
Despite the increased availability of public ECG datasets, label scarcity remains a central challenge
in the field. Self-supervised learning represents a promising way to alleviate this issue. In this
work, we put forward the first comprehensive assessment of self-supervised representation learning
from clinical 12-lead ECG data. To this end, we adapt state-of-the-art self-supervised methods
based on instance discrimination and latent forecasting to the ECG domain. In a first step, we learn
contrastive representations and evaluate their quality based on linear evaluation performance
on a recently established, comprehensive, clinical ECG classification task. In a second step,
we analyze the impact of self-supervised pretraining on finetuned ECG classifiers as compared
to purely supervised performance. For the best-performing method, an adaptation of contrastive
predictive coding, we find a linear evaluation performance only 0.5% below supervised performance.
For the finetuned models, we find improvements in downstream performance of roughly 1% compared
to supervised performance, label efficiency, as well as robustness against physiological noise.
This work clearly establishes the feasibility of extracting discriminative representations
from ECG data via self-supervised learning and the numerous advantages when finetuning such representations
on downstream tasks as compared to purely supervised training. As first comprehensive assessment
of its kind in the ECG domain carried out exclusively on publicly available datasets, we hope to establish
a first step towards reproducible progress in the rapidly evolving field of representation learning
for biosignals. 