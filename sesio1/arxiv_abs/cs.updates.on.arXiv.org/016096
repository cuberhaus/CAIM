While deep networks can learn complex functions such as classifiers, detectors, and trackers,
many applications require models that continually adapt to changing input distributions, changing
tasks, and changing environmental conditions. Indeed, this ability to continuously accrue knowledge
and use past experience to learn new tasks quickly in continual settings is one of the key properties
of an intelligent system. For complex and high-dimensional problems, simply updating the model
continually with standard learning algorithms such as gradient descent may result in slow adaptation.
Meta-learning can provide a powerful tool to accelerate adaptation yet is conventionally studied
in batch settings. In this paper, we study how meta-learning can be applied to tackle online problems
of this nature, simultaneously adapting to changing tasks and input distributions and meta-training
the model in order to adapt more quickly in the future. Extending meta-learning into the online setting
presents its own challenges, and although several prior methods have studied related problems,
they generally require a discrete notion of tasks, with known ground-truth task boundaries. Such
methods typically adapt to each task in sequence, resetting the model between tasks, rather than
adapting continuously across tasks. In many real-world settings, such discrete boundaries are
unavailable, and may not even exist. To address these settings, we propose a Fully Online Meta-Learning
(FOML) algorithm, which does not require any ground truth knowledge about the task boundaries and
stays fully online without resetting back to pre-trained weights. Our experiments show that FOML
was able to learn new tasks faster than the state-of-the-art online learning methods on Rainbow-MNIST,
CIFAR100 and CELEBA datasets. 