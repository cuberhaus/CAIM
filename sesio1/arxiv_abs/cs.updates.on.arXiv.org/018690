Existing keyframe-based motion synthesis mainly focuses on the generation of cyclic actions or
short-term motion, such as walking, running, and transitions between close postures. However,
these methods will significantly degrade the naturalness and diversity of the synthesized motion
when dealing with complex and impromptu movements, e.g., dance performance and martial arts. In
addition, current research lacks fine-grained control over the generated motion, which is essential
for intelligent human-computer interaction and animation creation. In this paper, we propose
a novel keyframe-based motion generation network based on multiple constraints, which can achieve
diverse dance synthesis via learned knowledge. Specifically, the algorithm is mainly formulated
based on the recurrent neural network (RNN) and the Transformer architecture. The backbone of our
network is a hierarchical RNN module composed of two long short-term memory (LSTM) units, in which
the first LSTM is utilized to embed the posture information of the historical frames into a latent
space, and the second one is employed to predict the human posture for the next frame. Moreover, our
framework contains two Transformer-based controllers, which are used to model the constraints
of the root trajectory and the velocity factor respectively, so as to better utilize the temporal
context of the frames and achieve fine-grained motion control. We verify the proposed approach
on a dance dataset containing a wide range of contemporary dance. The results of three quantitative
analyses validate the superiority of our algorithm. The video and qualitative experimental results
demonstrate that the complex motion sequences generated by our algorithm can achieve diverse and
smooth motion transitions between keyframes, even for long-term synthesis. 