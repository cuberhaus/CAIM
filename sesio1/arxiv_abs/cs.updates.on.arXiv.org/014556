The task of generating rich and fluent narratives that aptly describe the characteristics, trends,
and anomalies of time-series data is invaluable to the sciences (geology, meteorology, epidemiology)
or finance (trades, stocks, or sales and inventory). The efforts for time-series narration hitherto
are domain-specific and use predefined templates that offer consistency but lead to mechanical
narratives. We present TCube (Time-series-to-text), a domain-agnostic neural framework for
time-series narration, that couples the representation of essential time-series elements in
the form of a dense knowledge graph and the translation of said knowledge graph into rich and fluent
narratives through the transfer-learning capabilities of PLMs (Pre-trained Language Models).
TCube's design primarily addresses the challenge that lies in building a neural framework in the
complete paucity of annotated training data for time-series. The design incorporates knowledge
graphs as an intermediary for the representation of essential time-series elements which can be
linearized for textual translation. To the best of our knowledge, TCube is the first investigation
of the use of neural strategies for time-series narration. Through extensive evaluations, we show
that TCube can improve the lexical diversity of the generated narratives by up to 65.38% while still
maintaining grammatical integrity. The practicality and deployability of TCube is further validated
through an expert review (n=21) where 76.2% of participating experts wary of auto-generated narratives
favored TCube as a deployable system for time-series narration due to its richer narratives. Our
code-base, models, and datasets, with detailed instructions for reproducibility is publicly
hosted at https://github.com/Mandar-Sharma/TCube. 