Many software metrics are designed to measure aspects that are believed to be related to software
quality. Static software metrics, e.g., size, complexity and coupling are used in defect prediction
research as well as software quality models to evaluate software quality. While this indicates
a relationship between quality and software metrics, the extent of it is not well understood. Moreover,
recent studies found that complexity metrics may be unreliable indicators for understandability
of the source code. To explore this relationship, we leverage the intent of developers about what
constitutes a quality improvement in their own code base. We manually classify a randomized sample
of 2,533 commits from 54 Java open source projects as quality improving depending on the intent of
the developer by inspecting the commit message. We distinguish between perfective and corrective
maintenance via predefined guidelines and use this data as ground truth for the fine-tuning of a
state-of-the art deep learning model for natural language processing. We use the model to increase
our data set to 125,482 commits. Based on the resulting data set, we investigate the differences
in size and 14 static source code metrics between changes that increase quality, as indicated by
the developer, and other changes. We find that quality improving commits are smaller than other
commits. Perfective changes have a positive impact on static source code metrics while corrective
changes do tend to add complexity. Furthermore, we find that files which are the target of perfective
maintenance already have a lower median complexity than other files. Our study results provide
empirical evidence for which static source code metrics capture quality improvement from the developers
point of view. This has implications for program understanding as well as code smell detection and
recommender systems. 