We propose a new method for generating explanations with AI and a tool to test its expressive power
within a user interface. In order to bridge the gap between philosophy and human-computer interfaces,
we show a new approach for the generation of interactive explanations based on a sophisticated pipeline
of AI algorithms for structuring natural language documents into knowledge graphs, answering
questions effectively and satisfactorily. With this work we aim to prove that the philosophical
theory of explanations presented by Achinstein can be actually adapted for being implemented into
a concrete software application, as an interactive and illocutionary process of answering questions.
Specifically, our contribution is an approach to frame illocution in a computer-friendly way,
to achieve user-centrality with statistical question answering. Indeed, we frame the illocution
of an explanatory process as that mechanism responsible for anticipating the needs of the explainee
in the form of unposed, implicit, archetypal questions, hence improving the user-centrality of
the underlying explanatory process. Therefore, we hypothesise that if an explanatory process
is an illocutionary act of providing content-giving answers to questions, and illocution is as
we defined it, the more explicit and implicit questions can be answered by an explanatory tool, the
more usable its explanations. We tested our hypothesis with a user-study involving more than 60
participants, on two XAI-based systems, one for credit approval and one for heart disease prediction.
The results showed that increasing the illocutionary power of an explanatory tool can produce statistically
significant improvements on effectiveness. This, combined with a visible alignment between the
increments in effectiveness and satisfaction, suggests that our understanding of illocution
can be correct, giving evidence in favour of our theory. 