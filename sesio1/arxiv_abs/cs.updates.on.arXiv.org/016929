The rising popularity of online User-Generated-Content (UGC) in the form of streamed and shared
videos, has hastened the development of perceptual Video Quality Assessment (VQA) models, which
can be used to help optimize their delivery. Gaming videos, which are a relatively new type of UGC
videos, are created when skilled gamers post videos of their gameplay. These kinds of screenshots
of UGC gameplay videos have become extremely popular on major streaming platforms like YouTube
and Twitch. Synthetically-generated gaming content presents challenges to existing VQA algorithms,
including those based on natural scene/video statistics models. Synthetically generated gaming
content presents different statistical behavior than naturalistic videos. A number of studies
have been directed towards understanding the perceptual characteristics of professionally generated
gaming videos arising in gaming video streaming, online gaming, and cloud gaming. However, little
work has been done on understanding the quality of UGC gaming videos, and how it can be characterized
and predicted. Towards boosting the progress of gaming video VQA model development, we conducted
a comprehensive study of subjective and objective VQA models on UGC gaming videos. To do this, we
created a novel UGC gaming video resource, called the LIVE-YouTube Gaming video quality (LIVE-YT-Gaming)
database, comprised of 600 real UGC gaming videos. We conducted a subjective human study on this
data, yielding 18,600 human quality ratings recorded by 61 human subjects. We also evaluated a number
of state-of-the-art (SOTA) VQA models on the new database, including a new one, called GAME-VQP,
based on both natural video statistics and CNN-learned features. To help support work in this field,
we are making the new LIVE-YT-Gaming Database, publicly available through the link: https://live.ece.utexas.edu/research/LIVE-YT-Gaming/index.html
. 