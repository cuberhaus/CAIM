Central to a number of scholarly, regulatory, and public conversations about algorithmic accountability
is the question of who should have access to documentation that reveals the inner workings, intended
function, and anticipated consequences of algorithmic systems, potentially establishing new
routes for impacted publics to contest the operations of these systems. Currently, developers
largely have a monopoly on information about how their systems actually work and are incentivized
to maintain their own ignorance about aspects of how their systems affect the world. Increasingly,
legislators, regulators and advocates have turned to assessment documentation in order to address
the gap between the public's experience of algorithmic harms and the obligations of developers
to document and justify their design decisions. However, issues of standing and expertise currently
prevent publics from cohering around shared interests in preventing and redressing algorithmic
harms; as we demonstrate with multiple cases, courts often find computational harms non-cognizable
and rarely require developers to address material claims of harm. Constructed with a triadic accountability
relationship, algorithmic impact assessment regimes could alter this situation by establishing
procedural rights around public access to reporting and documentation. Developing a relational
approach to accountability, we argue that robust accountability regimes must establish opportunities
for publics to cohere around shared experiences and interests, and to contest the outcomes of algorithmic
systems that affect their lives. Furthermore, algorithmic accountability policies currently
under consideration in many jurisdictions must provide the public with adequate standing and opportunities
to access and contest the documentation provided by the actors and the judgments passed by the forum.
