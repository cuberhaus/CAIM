A promising approach for multi-microphone speech separation involves two deep neural networks
(DNN), where the predicted target speech from the first DNN is used to compute signal statistics
for time-invariant minimum variance distortionless response (MVDR) beamforming, and the MVDR
result is then used as extra features for the second DNN to predict target speech. Previous studies
suggested that the MVDR result can provide complementary information for the second DNN to better
predict target speech. However, on fixed-geometry arrays, both DNNs can take in, for example, the
real and imaginary (RI) components of the multi-channel mixture as features to leverage the spatial
and spectral information for enhancement. It is not explained clearly why the linear MVDR result
can be complementary and why it is still needed, considering that the DNNs and the beamformer use
the same input, and the DNNs perform non-linear filtering and could render the linear filtering
of MVDR unnecessary. Similarly, in monaural cases, one can replace the MVDR beamformer with a monaural
weighted prediction error (WPE) filter. Although the linear WPE filter and the DNNs use the same
mixture RI components as input, the WPE result is found to significantly improve the second DNN.
This study provides a novel explanation from the perspective of the low-distortion nature of such
algorithms, and finds that they can consistently improve phase estimation. Equipped with this
understanding, we investigate several low-distortion target estimation algorithms including
several beamformers, WPE, forward convolutive prediction, and their combinations, and use their
results as extra features to train the second network to achieve better enhancement. Evaluation
results on single- and multi-microphone speech dereverberation and enhancement tasks indicate
the effectiveness of the proposed approach, and the validity of the proposed view. 