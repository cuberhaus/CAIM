With the development of deep convolutional neural networks, medical image segmentation has achieved
a series of breakthroughs in recent years. However, the higher-performance convolutional neural
networks always mean numerous parameters and expensive computation costs, which will hinder the
applications in clinical scenarios. Meanwhile, the scarceness of large-scale annotated medical
image datasets further impedes the application of high-performance networks. To tackle these
problems, we propose Graph Flow, a novel comprehensive knowledge distillation method, to exploit
the cross-layer graph flow knowledge for both network-efficient and annotation-efficient medical
image segmentation. Specifically, our Graph Flow Distillation constructs a variation graph which
is employed to measure the flow of channel-wise salience features between different layers. Next,
the knowledge included in the variation graph is transferred from a well-trained cumbersome teacher
network to a non-trained compact student network. In addition, an unsupervised Paraphraser Module
is designed to refine the knowledge of the teacher network, which is also beneficial for the stabilization
of training procedure. Furthermore, we build a unified distillation framework by integrating
the adversarial distillation and the vanilla logits distillation, which can further promote the
final performance respectively. As a result, extensive experiments conducted on Gastric Cancer
Segmentation Dataset and Synapse Multi-organ Segmentation Dataset demonstrate the prominent
ability of our method which achieves state-of-the-art performance on these different-modality
and multi-category medical image datasets. Moreover, we demonstrate the effectiveness of our
Graph Flow through a new semi-supervised paradigm for dual-efficient medical image segmentation.
