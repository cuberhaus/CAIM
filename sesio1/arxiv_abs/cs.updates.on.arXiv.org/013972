There is a significant expansion in both volume and range of applications along with the concomitant
increase in the variety of data sources. These ever-expanding trends have highlighted the necessity
for more versatile analysis tools that offer greater opportunities for algorithmic developments
and computationally faster operations than the standard flat-view matrix approach. Tensors,
or multi-way arrays, provide such an algebraic framework which is naturally suited to data of such
large volume, diversity, and veracity. Indeed, the associated tensor decompositions have demonstrated
their potential in breaking the Curse of Dimensionality associated with traditional matrix methods,
where a necessary exponential increase in data volume leads to adverse or even intractable consequences
on computational complexity. A key tool underpinning multi-linear manipulation of tensors and
tensor networks is the standard Tensor Contraction Product (TCP). However, depending on the dimensionality
of the underlying tensors, the TCP also comes at the price of high computational complexity in tensor
manipulation. In this work, we resort to diagrammatic tensor network manipulation to calculate
such products in an efficient and computationally tractable manner, by making use of Tensor Train
decomposition (TTD). This has rendered the underlying concepts easy to perceive, thereby enhancing
intuition of the associated underlying operations, while preserving mathematical rigour. In
addition to bypassing the cumbersome mathematical multi-linear expressions, the proposed Tensor
Train Contraction Product model is shown to accelerate significantly the underlying computational
operations, as it is independent of tensor order and linear in the tensor dimension, as opposed to
performing the full computations through the standard approach (exponential in tensor order).
