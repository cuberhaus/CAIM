In this thesis, we address the problem of estimating the 6D pose of rigid objects from a single RGB
or RGB-D input image, assuming that 3D models of the objects are available. This problem is of great
importance to many application fields such as robotic manipulation, augmented reality, and autonomous
driving. First, we propose EPOS, a method for 6D object pose estimation from an RGB image. The key
idea is to represent an object by compact surface fragments and predict the probability distribution
of corresponding fragments at each pixel of the input image by a neural network. Each pixel is linked
with a data-dependent number of fragments, which allows systematic handling of symmetries, and
the 6D poses are estimated from the links by a RANSAC-based fitting method. EPOS outperformed all
RGB and most RGB-D and D methods on several standard datasets. Second, we present HashMatch, an RGB-D
method that slides a window over the input image and searches for a match against templates, which
are pre-generated by rendering 3D object models in different orientations. The method applies
a cascade of evaluation stages to each window location, which avoids exhaustive matching against
all templates. Third, we propose ObjectSynth, an approach to synthesize photorealistic images
of 3D object models for training methods based on neural networks. The images yield substantial
improvements compared to commonly used images of objects rendered on top of random photographs.
Fourth, we introduce T-LESS, the first dataset for 6D object pose estimation that includes 3D models
and RGB-D images of industry-relevant objects. Fifth, we define BOP, a benchmark that captures
the status quo in the field. BOP comprises eleven datasets in a unified format, an evaluation methodology,
an online evaluation system, and public challenges held at international workshops organized
at the ICCV and ECCV conferences. 