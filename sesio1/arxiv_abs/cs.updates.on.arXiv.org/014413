Reading code is an essential activity in software maintenance and evolution. Several studies with
human subjects have investigated how different factors, such as the employed programming constructs
and naming conventions, can impact code readability, i.e., what makes a program easier or harder
to read and apprehend by developers, and code legibility, i.e., what influences the ease of identifying
elements of a program. These studies evaluate readability and legibility by means of different
comprehension tasks and response variables. In this paper, we examine these tasks and variables
in studies that compare programming constructs, coding idioms, naming conventions, and formatting
guidelines, e.g., recursive vs. iterative code. To that end, we have conducted a systematic literature
review where we found 54 relevant papers. Most of these studies evaluate code readability and legibility
by measuring the correctness of the subjects' results (83.3%) or simply asking their opinions (55.6%).
Some studies (16.7%) rely exclusively on the latter variable.There are still few studies that monitor
subjects' physical signs, such as brain activation regions (5%). Moreover, our study shows that
some variables are multi-faceted. For instance, correctness can be measured as the ability to predict
the output of a program, answer questions about its behavior, or recall parts of it. These results
make it clear that different evaluation approaches require different competencies from subjects,
e.g., tracing the program vs. summarizing its goal vs. memorizing its text. To assist researchers
in the design of new studies and improve our comprehension of existing ones, we model program comprehension
as a learning activity by adapting a preexisting learning taxonomy. This adaptation indicates
that some competencies are often exercised in these evaluations whereas others are rarely targeted.
