Camouflage is a common visual phenomenon, which refers to hiding the foreground objects into the
background images, making them briefly invisible to the human eye. Previous work has typically
been implemented by an iterative optimization process. However, these methods struggle in 1) efficiently
generating camouflage images using foreground and background with arbitrary structure; 2) camouflaging
foreground objects to regions with multiple appearances (e.g. the junction of the vegetation and
the mountains), which limit their practical application. To address these problems, this paper
proposes a novel Location-free Camouflage Generation Network (LCG-Net) that fuse high-level
features of foreground and background image, and generate result by one inference. Specifically,
a Position-aligned Structure Fusion (PSF) module is devised to guide structure feature fusion
based on the point-to-point structure similarity of foreground and background, and introduce
local appearance features point-by-point. To retain the necessary identifiable features, a new
immerse loss is adopted under our pipeline, while a background patch appearance loss is utilized
to ensure that the hidden objects look continuous and natural at regions with multiple appearances.
Experiments show that our method has results as satisfactory as state-of-the-art in the single-appearance
regions and are less likely to be completely invisible, but far exceed the quality of the state-of-the-art
in the multi-appearance regions. Moreover, our method is hundreds of times faster than previous
methods. Benefitting from the unique advantages of our method, we provide some downstream applications
for camouflage generation, which show its potential. The related code and dataset will be released
at https://github.com/Tale17/LCG-Net. 