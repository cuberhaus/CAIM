The concept of meaningful human control has been proposed to address responsibility gaps and mitigate
them by establishing conditions that enable a proper attribution of responsibility for humans
(e.g., users, designers and developers, manufacturers, legislators). However, the relevant
discussions around meaningful human control have so far not resulted in clear requirements for
researchers, designers, and engineers. As a result, there is no consensus on how to assess whether
a designed AI system is under meaningful human control, making the practical development of AI-based
systems that remain under meaningful human control challenging. In this paper, we address the gap
between philosophical theory and engineering practice by identifying four actionable properties
which AI-based systems must have to be under meaningful human control. First, a system in which humans
and AI algorithms interact should have an explicitly defined domain of morally loaded situations
within which the system ought to operate. Second, humans and AI agents within the system should have
appropriate and mutually compatible representations. Third, responsibility attributed to a
human should be commensurate with that human's ability and authority to control the system. Fourth,
there should be explicit links between the actions of the AI agents and actions of humans who are aware
of their moral responsibility. We argue these four properties are necessary for AI systems under
meaningful human control, and provide possible directions to incorporate them into practice.
We illustrate these properties with two use cases, automated vehicle and AI-based hiring. We believe
these four properties will support practically-minded professionals to take concrete steps toward
designing and engineering for AI systems that facilitate meaningful human control and responsibility.
