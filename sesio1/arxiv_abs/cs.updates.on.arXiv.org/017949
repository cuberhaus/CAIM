Recent arguments that machine learning (ML) is facing a reproducibility and replication crisis
suggest that some published claims in ML research cannot be taken at face value. These concerns inspire
analogies to the replication crisis affecting the social and medical sciences. They also inspire
calls for the integration of statistical approaches to causal inference and predictive modeling.
A deeper understanding of what reproducibility concerns in supervised ML research have in common
with the replication crisis in experimental science puts the new concerns in perspective, and helps
researchers avoid "the worst of both worlds," where ML researchers begin borrowing methodologies
from explanatory modeling without understanding their limitations and vice versa. We contribute
a comparative analysis of concerns about inductive learning that arise in causal attribution as
exemplified in psychology versus predictive modeling as exemplified in ML. We identify themes
that re-occur in reform discussions, like overreliance on asymptotic theory and non-credible
beliefs about real-world data generating processes. We argue that in both fields, claims from learning
are implied to generalize outside the specific environment studied (e.g., the input dataset or
subject sample, modeling implementation, etc.) but are often impossible to refute due to undisclosed
sources of variance in the learning pipeline. In particular, errors being acknowledged in ML expose
cracks in long-held beliefs that optimizing predictive accuracy using huge datasets absolves
one from having to consider a true data generating process or formally represent uncertainty in
performance claims. We conclude by discussing risks that arise when sources of errors are misdiagnosed
and the need to acknowledge the role of human inductive biases in learning and reform. 