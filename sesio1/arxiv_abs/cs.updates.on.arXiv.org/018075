Natural Language Processing (NLP) is widely used to support the automation of different Requirements
Engineering (RE) tasks. Most of the proposed approaches start with various NLP steps that analyze
requirements statements, extract their linguistic information, and convert them to easy-to-process
representations, such as lists of features or embedding-based vector representations. These
NLP-based representations are usually used at a later stage as inputs for machine learning techniques
or rule-based methods. Thus, requirements representations play a major role in determining the
accuracy of different approaches. In this paper, we conducted a survey in the form of a systematic
literature mapping (classification) to find out (1) what are the representations used in RE tasks
literature, (2) what is the main focus of these works, (3) what are the main research directions in
this domain, and (4) what are the gaps and potential future directions. After compiling an initial
pool of 2,227 papers, and applying a set of inclusion/exclusion criteria, we obtained a final pool
containing 104 relevant papers. Our survey shows that the research direction has changed from the
use of lexical and syntactic features to the use of advanced embedding techniques, especially in
the last two years. Using advanced embedding representations has proved its effectiveness in most
RE tasks (such as requirement analysis, extracting requirements from reviews and forums, and semantic-level
quality tasks). However, representations that are based on lexical and syntactic features are
still more appropriate for other RE tasks (such as modeling and syntax-level quality tasks) since
they provide the required information for the rules and regular expressions used when handling
these tasks. In addition, we identify four gaps in the existing literature, why they matter, and
how future research can begin to address them. 