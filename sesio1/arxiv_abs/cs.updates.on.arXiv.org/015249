Intention decoding is an indispensable procedure in hands-free human-computer interaction (HCI).
Conventional eye-tracking system using single-model fixation duration possibly issues commands
ignoring users' real expectation. In the current study, an eye-brain hybrid brain-computer interface
(BCI) interaction system was introduced for intention detection through fusion of multi-modal
eye-track and ERP (a measurement derived from EEG) features. Eye-track and EEG data were recorded
from 64 healthy participants as they performed a 40-min customized free search task of a fixed target
icon among 25 icons. The corresponding fixation duration of eye-tracking and ERP were extracted.
Five previously-validated LDA-based classifiers (including RLDA, SWLDA, BLDA, SKLDA, and STDA)
and the widely-used CNN method were adopted to verify the efficacy of feature fusion from both offline
and pseudo-online analysis, and optimal approach was evaluated through modulating the training
set and system response duration. Our study demonstrated that the input of multi-modal eye-track
and ERP features achieved superior performance of intention detection in the single trial classification
of active search task. And compared with single-model ERP feature, this new strategy also induced
congruent accuracy across different classifiers. Moreover, in comparison with other classification
methods, we found that the SKLDA exhibited the superior performance when fusing feature in offline
test (ACC=0.8783, AUC=0.9004) and online simulation with different sample amount and duration
length. In sum, the current study revealed a novel and effective approach for intention classification
using eye-brain hybrid BCI, and further supported the real-life application of hands-free HCI
in a more precise and stable manner. 