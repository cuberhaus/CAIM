Underwater acoustic cameras are high potential devices for many applications in ecology, notably
for fisheries management and monitoring. However how to extract such data into high value information
without a time-consuming entire dataset reading by an operator is still a challenge. Moreover the
analysis of acoustic imaging, due to its low signal-to-noise ratio, is a perfect training ground
for experimenting with new approaches, especially concerning Deep Learning techniques. We present
hereby a novel approach that takes advantage of both CNN (Convolutional Neural Network) and classical
CV (Computer Vision) techniques, able to detect a generic class ''fish'' in acoustic video streams.
The pipeline pre-treats the acoustic images to extract 2 features, in order to localise the signals
and improve the detection performances. To ensure the performances from an ecological point of
view, we propose also a two-step validation, one to validate the results of the trainings and one
to test the method on a real-world scenario. The YOLOv3-based model was trained with data of fish
from multiple species recorded by the two common acoustic cameras, DIDSON and ARIS, including species
of high ecological interest, as Atlantic salmon or European eels. The model we developed provides
satisfying results detecting almost 80% of fish and minimizing the false positive rate, however
the model is much less efficient for eel detections on ARIS videos. The first CNN pipeline for fish
monitoring exploiting video data from two models of acoustic cameras satisfies most of the required
features. Many challenges are still present, such as the automation of fish species identification
through a multiclass model. 1 However the results point a new solution for dealing with complex data,
such as sonar data, which can also be reapplied in other cases where the signal-to-noise ratio is
a challenge. 