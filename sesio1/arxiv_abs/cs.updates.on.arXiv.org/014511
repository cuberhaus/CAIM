The spread of disinformation on social media platforms such as Facebook is harmful to society. This
harm may manifest as a gradual degradation of public discourse; but it can also take the form of sudden
dramatic events such as the recent insurrection on Capitol Hill. The platforms themselves are in
the best position to prevent the spread of disinformation, as they have the best access to relevant
data and the expertise to use it. However, mitigating disinformation is costly, not only for implementing
classification algorithms or employing manual detection, but also because limiting such highly
viral content impacts user growth and thus potential advertising revenue. Since the costs of harmful
content are borne by other entities, the platform will therefore have no incentive to exercise the
socially-optimal level of effort. This problem is similar to the problem of environmental regulation,
in which the costs of adverse events are not directly borne by a firm, the mitigation effort of a firm
is not observable, and the causal link between a harmful consequence and a specific failure is difficult
to prove. In the environmental regulation domain, one solution to this issue is to perform costly
monitoring to ensure that the firm takes adequate precautions according a specified rule. However,
classifying disinformation is performative, and thus a fixed rule becomes less effective over
time. Encoding our domain as a Markov decision process, we demonstrate that no penalty based on a
static rule, no matter how large, can incentivize adequate effort by the platform. Penalties based
on an adaptive rule can incentivize optimal effort, but counterintuitively, only if the regulator
sufficiently overreacts to harmful events by requiring a greater-than-optimal level of effort.
We therefore push for mechanisms that elicit platforms' costs of precautionary effort in order
to bypass such an overreaction. 