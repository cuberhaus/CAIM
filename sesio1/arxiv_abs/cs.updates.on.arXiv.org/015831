Knowledge in the human mind exhibits a dualistic vector/network nature. Modelling words as vectors
is key to natural language processing, whereas networks of word associations can map the nature
of semantic memory. We reconcile these paradigms - fragmented across linguistics, psychology
and computer science - by introducing FEature-Rich MUltiplex LEXical (FERMULEX) networks. This
novel framework merges structural similarities in networks and vector features of words, which
can be combined or explored independently. Similarities model heterogenous word associations
across semantic/syntactic/phonological aspects of knowledge. Words are enriched with multi-dimensional
feature embeddings including frequency, age of acquisition, length and polysemy. These aspects
enable unprecedented explorations of cognitive knowledge. Through CHILDES data, we use FERMULEX
networks to model normative language acquisition by 1000 toddlers between 18 and 30 months. Similarities
and embeddings capture word homophily via conformity, which measures assortative mixing via distance
and features. Conformity unearths a language kernel of frequent/polysemous/short nouns and verbs
key for basic sentence production, supporting recent evidence of children's syntactic constructs
emerging at 30 months. This kernel is invisible to network core-detection and feature-only clustering:
It emerges from the dual vector/network nature of words. Our quantitative analysis reveals two
key strategies in early word learning. Modelling word acquisition as random walks on FERMULEX topology,
we highlight non-uniform filling of communicative developmental inventories (CDIs). Conformity-based
walkers lead to accurate (75%), precise (55%) and partially well-recalled (34%) predictions of
early word learning in CDIs, providing quantitative support to previous empirical findings and
developmental theories. 