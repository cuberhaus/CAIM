Conversational Agents (CAs) powered with deep language models (DLMs) have shown tremendous promise
in the domain of mental health. Prominently, the CAs have been used to provide informational or therapeutic
services to patients. However, the utility of CAs to assist in mental health triaging has not been
explored in the existing work as it requires a controlled generation of follow-up questions (FQs),
which are often initiated and guided by the mental health professionals (MHPs) in clinical settings.
In the context of depression, our experiments show that DLMs coupled with process knowledge in a
mental health questionnaire generate 12.54% and 9.37% better FQs based on similarity and longest
common subsequence matches to questions in the PHQ-9 dataset respectively, when compared with
DLMs without process knowledge support. Despite coupling with process knowledge, we find that
DLMs are still prone to hallucination, i.e., generating redundant, irrelevant, and unsafe FQs.
We demonstrate the challenge of using existing datasets to train a DLM for generating FQs that adhere
to clinical process knowledge. To address this limitation, we prepared an extended PHQ-9 based
dataset, PRIMATE, in collaboration with MHPs. PRIMATE contains annotations regarding whether
a particular question in the PHQ-9 dataset has already been answered in the user's initial description
of the mental health condition. We used PRIMATE to train a DLM in a supervised setting to identify
which of the PHQ-9 questions can be answered directly from the user's post and which ones would require
more information from the user. Using performance analysis based on MCC scores, we show that PRIMATE
is appropriate for identifying questions in PHQ-9 that could guide generative DLMs towards controlled
FQ generation suitable for aiding triaging. Dataset created as a part of this research: https://github.com/primate-mh/Primate2022
