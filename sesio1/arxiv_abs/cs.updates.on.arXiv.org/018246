The formation of eyes led to the big bang of evolution. The dynamics changed from a primitive organism
waiting for the food to come into contact for eating food being sought after by visual sensors. The
human eye is one of the most sophisticated developments of evolution, but it still has defects. Humans
have evolved a biological perception algorithm capable of driving cars, operating machinery,
piloting aircraft, and navigating ships over millions of years. Automating these capabilities
for computers is critical for various applications, including self-driving cars, augmented reality,
and architectural surveying. Near-field visual perception in the context of self-driving cars
can perceive the environment in a range of $0-10$ meters and 360{\deg} coverage around the vehicle.
It is a critical decision-making component in the development of safer automated driving. Recent
advances in computer vision and deep learning, in conjunction with high-quality sensors such as
cameras and LiDARs, have fueled mature visual perception solutions. Until now, far-field perception
has been the primary focus. Another significant issue is the limited processing power available
for developing real-time applications. Because of this bottleneck, there is frequently a trade-off
between performance and run-time efficiency. We concentrate on the following issues in order to
address them: 1) Developing near-field perception algorithms with high performance and low computational
complexity for various visual perception tasks such as geometric and semantic tasks using convolutional
neural networks. 2) Using Multi-Task Learning to overcome computational bottlenecks by sharing
initial convolutional layers between tasks and developing optimization strategies that balance
tasks. 