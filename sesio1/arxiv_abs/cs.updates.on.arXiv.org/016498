Due to climate change, we can observe a recent surge of natural disasters all around the world. These
disasters are causing disastrous impact on both nature and human lives. Economic losses are getting
greater due to the hurricanes. Quick and prompt response of the rescue teams are crucial in saving
human lives and reducing economic cost. Deep learning based computer vision techniques can help
in scene understanding, and help rescue teams with precise damage assessment. Semantic segmentation,
an active research area in computer vision, can put labels to each pixel of an image, and therefore
can be a valuable arsenal in the effort of reducing the impacts of hurricanes. Unfortunately, available
datasets for natural disaster damage assessment lack detailed annotation of the affected areas,
and therefore do not support the deep learning models in total damage assessment. To this end, we
introduce the RescueNet, a high resolution post disaster dataset, for semantic segmentation to
assess damages after natural disasters. The RescueNet consists of post disaster images collected
after Hurricane Michael. The data is collected using Unmanned Aerial Vehicles (UAVs) from several
areas impacted by the hurricane. The uniqueness of the RescueNet comes from the fact that this dataset
provides high resolution post-disaster images and comprehensive annotation of each image. While
most of the existing dataset offer annotation of only part of the scene, like building, road, or river,
RescueNet provides pixel level annotation of all the classes including building, road, pool, tree,
debris, and so on. We further analyze the usefulness of the dataset by implementing state-of-the-art
segmentation models on the RescueNet. The experiments demonstrate that our dataset can be valuable
in further improvement of the existing methodologies for natural disaster damage assessment.
