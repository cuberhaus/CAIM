The Superfacility model is designed to leverage HPC for experimental science. It is more than simply
a model of connected experiment, network, and HPC facilities; it encompasses the full ecosystem
of infrastructure, software, tools, and expertise needed to make connected facilities easy to
use. The three-year Lawrence Berkeley National Laboratory (LBNL) Superfacility project was initiated
in 2019 to coordinate work being performed at LBNL to support this model, and to provide a coherent
and comprehensive set of science requirements to drive existing and new work. A key component of
the project was the in-depth engagements with eight science teams that represent challenging use
cases across the DOE Office of Science. By the close of the project, we met our project goal by enabling
our science application engagements to demonstrate automated pipelines that analyze data from
remote facilities at large scale, without routine human intervention. In several cases, we have
gone beyond demonstrations and now provide production-level services. To achieve this goal, the
Superfacility team developed tools, infrastructure, and policies for near-real-time computing
support, dynamic high-performance networking, data management and movement tools, API-driven
automation, HPC-scale notebooks via Jupyter, authentication using Federated Identity and container-based
edge services supported. The lessons we learned during this project provide a valuable model for
future large, complex, cross-disciplinary collaborations. There is a pressing need for a coherent
computing infrastructure across national facilities, and LBNL's Superfacility project is a unique
model for success in tackling the challenges that will be faced in hardware, software, policies,
and services across multiple science domains. 