As one of the most pervasive applications of machine learning, recommender systems are playing
an important role on assisting human decision making. The satisfaction of users and the interests
of platforms are closely related to the quality of the generated recommendation results. However,
as a highly data-driven system, recommender system could be affected by data or algorithmic bias
and thus generate unfair results, which could weaken the reliance of the systems. As a result, it
is crucial to address the potential unfairness problems in recommendation settings. Recently,
there has been growing attention on fairness considerations in recommender systems with more and
more literature on approaches to promote fairness in recommendation. However, the studies are
rather fragmented and lack a systematic organization, thus making it difficult to penetrate for
new researchers to the domain. This motivates us to provide a systematic survey of existing works
on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation
literature. It first presents a brief introduction about fairness in basic machine learning tasks
such as classification and ranking in order to provide a general overview of fairness research,
as well as introduce the more complex situations and challenges that need to be considered when studying
fairness in recommender systems. After that, the survey will introduce fairness in recommendation
with a focus on the taxonomies of current fairness definitions, the typical techniques for improving
fairness, as well as the datasets for fairness studies in recommendation. The survey also talks
about the challenges and opportunities in fairness research with the hope of promoting the fair
recommendation research area and beyond. 