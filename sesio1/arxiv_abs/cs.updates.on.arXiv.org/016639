The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to
regress a density map, requiring point-level annotations. However, annotating each person with
a point is an expensive and laborious process. During the testing phase, the point-level annotations
are not considered to evaluate the counting accuracy, which means the point-level annotations
are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just
rely on count-level annotations, a more economical way of labeling. Current weakly-supervised
counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm.
However, having limited receptive fields for context modeling is an intrinsic limitation of these
weakly-supervised CNN-based methods. These methods thus can not achieve satisfactory performance,
with limited applications in the real-word. The Transformer is a popular sequence-to-sequence
prediction model in NLP, which contains a global receptive field. In this paper, we propose TransCrowd,
which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count
based on Transformer. We observe that the proposed TransCrowd can effectively extract the semantic
crowd information by using the self-attention mechanism of Transformer. To the best of our knowledge,
this is the first work to adopt a pure Transformer for crowd counting research. Experiments on five
benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared
with all the weakly-supervised CNN-based counting methods and gains highly competitive counting
performance compared with some popular fully-supervised counting methods. An implementation
of our method is available at https://github.com/dk-liang/TransCrowd 