The past decade has seen a substantial rise in the amount of mis- and disinformation online, from
targeted disinformation campaigns to influence politics, to the unintentional spreading of misinformation
about public health. This development has spurred research in the area of automatic fact checking,
from approaches to detect check-worthy claims and determining the stance of tweets towards claims,
to methods to determine the veracity of claims given evidence documents. These automatic methods
are often content-based, using natural language processing methods, which in turn utilise deep
neural networks to learn higher-order features from text in order to make predictions. As deep neural
networks are black-box models, their inner workings cannot be easily explained. At the same time,
it is desirable to explain how they arrive at certain decisions, especially if they are to be used
for decision making. While this has been known for some time, the issues this raises have been exacerbated
by models increasing in size, and by EU legislation requiring models to be used for decision making
to provide explanations, and, very recently, by legislation requiring online platforms operating
in the EU to provide transparent reporting on their services. Despite this, current solutions for
explainability are still lacking in the area of fact checking. This thesis presents my research
on automatic fact checking, including claim check-worthiness detection, stance detection and
veracity prediction. Its contributions go beyond fact checking, with the thesis proposing more
general machine learning solutions for natural language processing in the area of learning with
limited labelled data. Finally, the thesis presents some first solutions for explainable fact
checking. 