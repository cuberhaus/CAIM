We propose a new method for generating explanations with Artificial Intelligence (AI) and a tool
to test its expressive power within a user interface. In order to bridge the gap between philosophy
and human-computer interfaces, we show a new approach for the generation of interactive explanations
based on a sophisticated pipeline of AI algorithms for structuring natural language documents
into knowledge graphs, answering questions effectively and satisfactorily. With this work we
aim to prove that the philosophical theory of explanations presented by Achinstein can be actually
adapted for being implemented into a concrete software application, as an interactive and illocutionary
process of answering questions. Specifically, our contribution is an approach to frame illocution
in a computer-friendly way, to achieve user-centrality with statistical question answering.
In fact, we frame illocution, in an explanatory process, as that mechanism responsible for anticipating
the needs of the explainee in the form of unposed, implicit, archetypal questions, hence improving
the user-centrality of the underlying explanatory process. More precisely, we hypothesise that
given an arbitrary explanatory process, increasing its goal-orientedness and degree of illocution
results in the generation of more usable (as per ISO 9241-210) explanations. We tested our hypotheses
with a user-study involving more than 60 participants, on two XAI-based systems, one for credit
approval (finance) and one for heart disease prediction (healthcare). The results showed that
our proposed solution produced a statistically significant improvement (hence with a p-value
lower than 0.05) on effectiveness. This, combined with a visible alignment between the increments
in effectiveness and satisfaction, suggests that our understanding of illocution can be correct,
giving evidence in favour of our theory. 