Despite recent progress in developing animatable full-body avatars, realistic modeling of clothing
- one of the core aspects of human self-expression - remains an open challenge. State-of-the-art
physical simulation methods can generate realistically behaving clothing geometry at interactive
rate. Modeling photorealistic appearance, however, usually requires physically-based rendering
which is too expensive for interactive applications. On the other hand, data-driven deep appearance
models are capable of efficiently producing realistic appearance, but struggle at synthesizing
geometry of highly dynamic clothing and handling challenging body-clothing configurations.
To this end, we introduce pose-driven avatars with explicit modeling of clothing that exhibit both
realistic clothing dynamics and photorealistic appearance learned from real-world data. The
key idea is to introduce a neural clothing appearance model that operates on top of explicit geometry:
at train time we use high-fidelity tracking, whereas at animation time we rely on physically simulated
geometry. Our key contribution is a physically-inspired appearance network, capable of generating
photorealistic appearance with view-dependent and dynamic shadowing effects even for unseen
body-clothing configurations. We conduct a thorough evaluation of our model and demonstrate diverse
animation results on several subjects and different types of clothing. Unlike previous work on
photorealistic full-body avatars, our approach can produce much richer dynamics and more realistic
deformations even for loose clothing. We also demonstrate that our formulation naturally allows
clothing to be used with avatars of different people while staying fully animatable, thus enabling,
for the first time, photorealistic avatars with novel clothing. 