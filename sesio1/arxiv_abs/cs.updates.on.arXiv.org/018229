By providing unprecedented access to computational resources, cloud computing has enabled rapid
growth in technologies such as machine learning, the computational demands of which incur a high
energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for
better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable
access to measurements of this information, precluding development of actionable tactics. Cloud
providers presenting information about software carbon intensity to users is a fundamental stepping
stone towards minimizing emissions. In this paper, we provide a framework for measuring software
carbon intensity, and propose to measure operational carbon emissions by using location-based
and time-specific marginal emissions data per energy unit. We provide measurements of operational
software carbon intensity for a set of modern models for natural language processing and computer
vision, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language
model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud
compute platform: using cloud instances in different geographic regions, using cloud instances
at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity
is above a certain threshold. We confirm previous results that the geographic region of the data
center plays a significant role in the carbon intensity for a given cloud instance, and find that
choosing an appropriate region can have the largest operational emissions reduction impact. We
also show that the time of day has notable impact on operational software carbon intensity. Finally,
we conclude with recommendations for how machine learning practitioners can use software carbon
intensity information to reduce environmental impact. 