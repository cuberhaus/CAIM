The spread of disinformation on social platforms is harmful to society. This harm may manifest as
a gradual degradation of public discourse; but it can also take the form of sudden dramatic events
such as the 2021 insurrection on Capitol Hill. The platforms themselves are in the best position
to prevent the spread of disinformation, as they have the best access to relevant data and the expertise
to use it. However, mitigating disinformation is costly, not only for implementing detection algorithms
or employing manual effort, but also because limiting such highly viral content impacts user engagement
and potential advertising revenue. Since the costs of harmful content are borne by other entities,
the platform will therefore have no incentive to exercise the socially-optimal level of effort.
This problem is similar to that of environmental regulation, in which the costs of adverse events
are not directly borne by a firm, the mitigation effort of a firm is not observable, and the causal
link between a harmful consequence and a specific failure is difficult to prove. For environmental
regulation, one solution is to perform costly monitoring to ensure that the firm takes adequate
precautions according to a specified rule. However, a fixed rule for classifying disinformation
becomes less effective over time, as bad actors can learn to sequentially and strategically bypass
it. Encoding our domain as a Markov decision process, we demonstrate that no penalty based on a static
rule, no matter how large, can incentivize optimal effort. Penalties based on an adaptive rule can
incentivize optimal effort, but counter-intuitively, only if the regulator sufficiently overreacts
to harmful events by requiring a greater-than-optimal level of effort. We offer novel insights
for the effective regulation of social platforms, highlight inherent challenges, and discuss
promising avenues for future work. 