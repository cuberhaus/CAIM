At the core of insurance business lies classification between risky and non-risky insureds, actuarial
fairness meaning that risky insureds should contribute more and pay a higher premium than non-risky
or less-risky ones. Actuaries, therefore, use econometric or machine learning techniques to classify,
but the distinction between a fair actuarial classification and "discrimination" is subtle. For
this reason, there is a growing interest about fairness and discrimination in the actuarial community
Lindholm, Richman, Tsanakas, and Wuthrich (2022). Presumably, non-sensitive characteristics
can serve as substitutes or proxies for protected attributes. For example, the color and model of
a car, combined with the driver's occupation, may lead to an undesirable gender bias in the prediction
of car insurance prices. Surprisingly, we will show that debiasing the predictor alone may be insufficient
to maintain adequate accuracy (1). Indeed, the traditional pricing model is currently built in
a two-stage structure that considers many potentially biased components such as car or geographic
risks. We will show that this traditional structure has significant limitations in achieving fairness.
For this reason, we have developed a novel pricing model approach. Recently some approaches have
Blier-Wong, Cossette, Lamontagne, and Marceau (2021); Wuthrich and Merz (2021) shown the value
of autoencoders in pricing. In this paper, we will show that (2) this can be generalized to multiple
pricing factors (geographic, car type), (3) it perfectly adapted for a fairness context (since
it allows to debias the set of pricing components): We extend this main idea to a general framework
in which a single whole pricing model is trained by generating the geographic and car pricing components
needed to predict the pure premium while mitigating the unwanted bias according to the desired metric.
