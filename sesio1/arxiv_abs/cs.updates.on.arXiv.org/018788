Human beings use compositionality to generalise from past experiences to actual or fictive, novel
experiences. To do so, we separate our experiences into fundamental atomic components. These atomic
components can then be recombined in novel ways to support our ability to imagine and engage with
novel experiences. We frame this as the ability to learn to generalise compositionally. And, we
will refer to behaviours making use of this ability as compositional learning behaviours (CLBs).
A central problem to learning CLBs is the resolution of a binding problem (BP) (by learning to, firstly,
segregate the supportive stimulus components from the observation of multiple stimuli, and then,
combine them in a single episodic experience). While it is another feat of intelligence that human
beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order
to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark
to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP.
We take inspiration from the language emergence and grounding framework of referential games and
propose a meta-learning extension of referential games, entitled Meta-Referential Games, and
use this framework to build our benchmark, that we name Symbolic Behaviour Benchmark (S2B). While
it has the potential to test for more symbolic behaviours, rather than solely CLBs, in the present
paper, though, we solely focus on the single-agent language grounding task that tests for CLBs.
We provide baseline results for it, using state-of-the-art RL agents, and show that our proposed
benchmark is a compelling challenge that we hope will spur the research community towards developing
more capable artificial agents. 