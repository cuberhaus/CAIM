This article surveys the landscape of semiconductor materials and devices research for the acceleration
of machine learning (ML) algorithms. We observe a disconnect between the semiconductor and device
physics and engineering communities, and the digital logic and computer hardware architecture
communities. The article first provides an overview of the principles of computational complexity
and fundamental physical limits to computing and their relation to physical systems. The article
then provides an introduction to ML by presenting three key components of ML systems: representation,
evaluation, and optimisation. The article then discusses and provides examples of the application
of emerging technologies from the demiconductor and device physics domains as solutions to computational
problems, alongside a brief overview of emerging devices for computing applications. The article
then reviews the landscape of ML accelerators, comparing fixed-function and reprogrammable digital
logic with novel devices such as memristors, resistive memories, magnetic memories, and probabilistic
bits. We observe broadly lower performance of ML accelerators based on novel devices and materials
when compared to those based on digital complimentary metal-oxide semiconductor (CMOS) technology,
particularly in the MNIST optical character recognition task, a common ML benchmark, and also highlight
the lack of a trend of progress in approaches based on novel materials and devices. Lastly, the article
proposes figures of merit for meaningful evaluation and comparison of different ML implementations
in the hope of fostering a dialogue between the materials science, device physics, digital logic,
and computer architecture communities by providing a common frame of reference for their work.
